{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals:\n",
    "- Usable taz files\n",
    "- usable skims\n",
    "- trip tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openmatrix as omx\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_omx_as_dict(infile_path):\n",
    "    store_dict = {}\n",
    "    with omx.open_file(infile_path,'r') as f:\n",
    "        for name in f.list_matrices():\n",
    "            store_dict[name] = np.array(f[name])\n",
    "    return store_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip tables\n",
    "# drop zone with SPRT_ID = 300, trip table index is 210\n",
    "\n",
    "purpose_dict = {\n",
    "    'HBW': 'HBW',\n",
    "    'HBPB':'HBO',\n",
    "    'HBSR':'HBO',\n",
    "    'NHBO':'NHB',\n",
    "    'NHBW':'NHB',\n",
    "    'HBSc_preKto8':'HBSc1',\n",
    "    'HBSc_HighSchool':'HBSc2',\n",
    "    'HBSc_CollegeComm':'HBSc3'\n",
    "}\n",
    "\n",
    "modes = ['Bike','Walk', 'WAT', 'SOV', 'HOV2p', 'HOV3p',\n",
    "        'DAT_Boat','DAT_CR','DAT_LB','DAT_RT',\n",
    "        'DET_Boat','DET_CR','DET_LB','DET_RT']\n",
    "\n",
    "# pre-mode-choice trip tables: eliminate unused zone, pre-mode choice, combine purposes (HBW, HBO, NHB, HBSc1, HBSc2, HBSc3), fix matrix names\n",
    "for year in ['2018','2040']:\n",
    "    out_dir = r'C:\\Users\\xchang\\Documents\\Seaport_data\\data\\\\' + year\n",
    "    if year == '2018':\n",
    "        in_dir = 'X:/Share/OAK/Transfer/XChang/2018 Base/OMX/'\n",
    "    else: in_dir = 'X:/Share/OAK/Transfer/XChang/2040 No Build/OMX/'\n",
    "    \n",
    "    skim = {}\n",
    "    \n",
    "    raw = {}\n",
    "    \n",
    "    # market segments: PK = AM+PM, OP = MD+NT; 1: withAutoOnly, 0: wAuto - withAutoOnly\n",
    "    # format: 0_PK\n",
    "    for tod in ['AM','MD','PM','NT']:\n",
    "        tt1_tables = {}\n",
    "        tt0_tables = {}\n",
    "        tt1 = store_omx_as_dict(glob.glob(in_dir+ f'Seaport_Final*{tod}*withAutoOnly*.omx')[0])\n",
    "        tt_all = store_omx_as_dict(glob.glob(in_dir+ f'Seaport_Final*{tod}*Zero*.omx')[0])\n",
    "        for purpose in purpose_dict:\n",
    "            try:\n",
    "                tt1_tables[purpose_dict[purpose]] += sum([tt1[f\"Sum of '{purpose}_{mode}'\"] for mode in modes])\n",
    "            except: tt1_tables[purpose_dict[purpose]] = sum([tt1[f\"Sum of '{purpose}_{mode}'\"]  for mode in modes])\n",
    "            \n",
    "            try:\n",
    "                tt0_tables[purpose_dict[purpose]] += sum([tt_all[f\"Sum of '{purpose}_{mode}'\"]  for mode in modes]) - sum([tt1[f\"Sum of '{purpose}_{mode}'\"] for mode in modes])\n",
    "            except: tt0_tables[purpose_dict[purpose]] = sum([tt_all[f\"Sum of '{purpose}_{mode}'\"]  for mode in modes]) - sum([tt1[f\"Sum of '{purpose}_{mode}'\"] for mode in modes])\n",
    "        raw[tod] = {'0Auto': tt0_tables, 'wAuto': tt1_tables}\n",
    "    \n",
    "\n",
    "    raw['PK'] = {'0Auto':{purpose: (raw['AM']['0Auto'][purpose] + raw['PM']['0Auto'][purpose]) for purpose in tt0_tables}, 'wAuto':{purpose: (raw['AM']['wAuto'][purpose] + raw['PM']['wAuto'][purpose]) for purpose in tt0_tables}}\n",
    "    raw['OP'] = {'0Auto':{purpose: (raw['MD']['0Auto'][purpose] + raw['NT']['0Auto'][purpose]) for purpose in tt0_tables}, 'wAuto':{purpose: (raw['MD']['wAuto'][purpose] + raw['NT']['wAuto'][purpose]) for purpose in tt0_tables}}\n",
    "    \n",
    "    # format: purpose_peak_vehicle\n",
    "    for peak in ['PK','OP']:\n",
    "        for veh in raw[peak]:\n",
    "            for purpose in raw[peak][veh]:\n",
    "                skim[f'{purpose}_{peak}_{veh}'] = raw[peak][veh][purpose]\n",
    "                \n",
    "    with omx.open_file(out_dir+'/pre_MC_trip_6_purposes.omx', 'w') as tt:\n",
    "        for key in skim:\n",
    "            tt[key] = np.delete(np.delete(skim[key],210,0), 210, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xchang\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tables\\path.py:157: NaturalNameWarning: object name is not a valid Python identifier: 'Auto_Toll (Skim)'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "C:\\Users\\xchang\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tables\\path.py:157: NaturalNameWarning: object name is not a valid Python identifier: 'Length (Skim)'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "# skims\n",
    "skim_terms =  {'AM_A_DAT_for_Boat_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'AM_A_DAT_for_CommRail_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'AM_A_DAT_for_LocalBus_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'AM_A_DAT_for_Rapid_Transit_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'AM_SOV_skim_SPRTDST.omx' : ['Auto_Toll (Skim)', 'CongTime', 'Length (Skim)', 'TerminalTimes'],\n",
    " 'AM_WAT_for_All_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'MD_A_DAT_for_Boat_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'MD_A_DAT_for_CommRail_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'MD_A_DAT_for_LocalBus_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'MD_A_DAT_for_Rapid_Transit_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'MD_SOV_skim_SPRTDST.omx' : ['Auto_Toll (Skim)', 'CongTime', 'Length (Skim)', 'TerminalTimes'],\n",
    " 'MD_WAT_for_All_tr_skim_SPRTDST.omx' : ['Total_Cost', 'Total_IVTT', 'Total_OVTT'],\n",
    " 'WalkSkim_SPRTDST.omx' : ['Length (Skim)', 'OneMileorLess', 'WalkTime'],\n",
    " 'BikeSkim_SPRTDST.omx' : ['BikeTime', 'Length (Skim)', 'OneMileorLess']}\n",
    "\n",
    "in_dir = 'X:/Share/OAK/Transfer/XChang/2018 Base/OMX/'\n",
    "out_dir = r'C:\\Users\\xchang\\Documents\\Seaport_data\\2018\\skims\\\\'\n",
    "for fn in skim_terms:\n",
    "    with omx.open_file(in_dir + fn, 'r') as in_skim, omx.open_file(out_dir + fn, 'w') as out_skim:\n",
    "        for term in skim_terms[fn]:\n",
    "            out_skim[term] = in_skim[f\"Sum of '{term}'\"]\n",
    "            \n",
    "in_dir = 'X:/Share/OAK/Transfer/XChang/2040 No Build/OMX/'\n",
    "out_dir = r'C:\\Users\\xchang\\Documents\\Seaport_data\\2040\\skims\\\\'\n",
    "for fn in skim_terms:\n",
    "    with omx.open_file(in_dir + fn, 'r') as in_skim, omx.open_file(out_dir + fn, 'w') as out_skim:\n",
    "        for term in skim_terms[fn]:\n",
    "            out_skim[term] = in_skim[f\"Sum of '{term}'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAZ 2018\n",
    "# drop zone with SPRT_ID = 300, trip table index is 210\n",
    "\n",
    "skim_mapping = pd.read_csv(r\"C:\\Users\\xchang\\OneDrive - Cambridge Systematics\\Documents\\Seaport\\skim_zone_mapping.csv\")\n",
    "\n",
    "TAZ_crosswalk = pd.read_csv(r\"X:\\Share\\OAK\\Transfer\\XChang\\MA_TAZs_with_Seaport_table.txt\")[['MA_TAZ_ID','SPRT_ID']]\n",
    "in_dir = r'C:\\Users\\xchang\\Documents\\FM_data\\data\\2016\\\\'\n",
    "out_dir = r'C:\\Users\\xchang\\Documents\\Seaport_data\\2018\\\\'\n",
    "#zonal\n",
    "f = 'taz_zonal.csv'\n",
    "df = pd.read_csv(in_dir + f).merge(TAZ_crosswalk, left_on = 'ID', right_on = 'MA_TAZ_ID').groupby('SPRT_ID').agg(\n",
    "    {'Area': 'sum', 'Tot_Pop': 'sum', 'Tot_Emp':'sum', 'HH_Pop':'sum','HH':'sum','VehiclesPerWorker':'mean',\n",
    "    'AM_wacc_fact':'mean','MD_wacc_fact':'mean','Hwy Prod Term Time':'mean'})\n",
    "df.drop(300, inplace = True)\n",
    "df.to_csv(out_dir + f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parking\n",
    "f = \"Land_Use_Parking_Costs.csv\"\n",
    "parking = pd.read_csv(in_dir + f).merge(TAZ_crosswalk, left_on = 'ID', right_on = 'MA_TAZ_ID').groupby('SPRT_ID').mean()['Daily Parking Cost']\n",
    "\n",
    "df = pd.DataFrame(0, index = skim_mapping['SPRT_ID'], columns = ['Daily Parking Cost'])\n",
    "df['Daily Parking Cost'] = parking\n",
    "df.fillna(0).to_csv(out_dir + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taz\n",
    "\n",
    "f = 'SW_TAZ_2010.csv'\n",
    "df = pd.read_csv(in_dir + f).merge(TAZ_crosswalk, left_on = 'ID_FOR_CS', right_on = 'MA_TAZ_ID').groupby('SPRT_ID').agg(\n",
    "{'BOSTON_NB':'first', 'TOWN': lambda x: '; '.join(x.unique())})\n",
    "df.to_csv(out_dir + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taz interstate\n",
    "interstate = pd.read_csv(r\"C:\\Egnyte\\Shared\\Projects\\190077\\1 - Existing Conditions\\4 - CTPS\\gis\\jayne\\TAZ_SPRT_interstate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "interstate.drop('FID',axis = 1).set_index('SPRT_ID').to_csv(r'C:\\Users\\xchang\\Documents\\Seaport_data\\taz_interstate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAZ 2040\n",
    "# drop zone with SPRT_ID = 300, trip table index is 210\n",
    "\n",
    "skim_mapping = pd.read_csv(r\"C:\\Users\\xchang\\OneDrive - Cambridge Systematics\\Documents\\Seaport\\skim_zone_mapping.csv\")\n",
    "\n",
    "TAZ_crosswalk = pd.read_csv(r\"X:\\Share\\OAK\\Transfer\\XChang\\MA_TAZs_with_Seaport_table.txt\")[['MA_TAZ_ID','SPRT_ID']]\n",
    "in_dir = r'C:\\Users\\xchang\\Documents\\FM_data\\data\\2040\\\\'\n",
    "out_dir = r'C:\\Users\\xchang\\Documents\\Seaport_data\\2040\\\\'\n",
    "\n",
    "#zonal\n",
    "f = 'TAZ_zonal_2040.csv'\n",
    "df = pd.read_csv(in_dir + f).merge(TAZ_crosswalk, left_on = 'ID', right_on = 'MA_TAZ_ID').groupby('SPRT_ID').agg(\n",
    "    {'Area': 'sum', 'Acc_PEV':'mean','Egr_PEV':'mean','Tot_Pop': 'sum', 'Tot_Emp':'sum', 'HH_Pop':'sum','HH':'sum','VehiclesPerWorker':'mean',\n",
    "    'AM_wacc_fact':'mean','MD_wacc_fact':'mean','Hwy Prod Term Time':'mean'})\n",
    "df.drop(300, inplace = True)\n",
    "df.to_csv(out_dir + f)\n",
    "\n",
    "\n",
    "# parking\n",
    "f = \"Land_Use_Parking_Costs.csv\"\n",
    "parking = pd.read_csv(in_dir + f).merge(TAZ_crosswalk, left_on = 'ID', right_on = 'MA_TAZ_ID').groupby('SPRT_ID').mean()['Daily Parking Cost']\n",
    "df = pd.DataFrame(0, index = skim_mapping['SPRT_ID'], columns = ['Daily Parking Cost'])\n",
    "df['Daily Parking Cost'] = parking\n",
    "df.fillna(0).to_csv(out_dir + f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAZ_crosswalk = pd.read_csv(r\"X:\\Share\\OAK\\Transfer\\XChang\\MA_TAZs_with_Seaport_table.txt\")\n",
    "\n",
    "\n",
    "# taz\n",
    "\n",
    "f = 'SW_TAZ_2010.csv'\n",
    "\n",
    "# want TAZ_crosswalk where SPRT_ID is in skim_mapping['SPRT_ID']\n",
    "# keep town information in TAZ_crosswalk\n",
    "# merge BOSTON_NB info from f\n",
    "\n",
    "town = skim_mapping[['SPRT_ID']].merge(TAZ_crosswalk.groupby('SPRT_ID').first()[['TOWN']].reset_index())\n",
    "nb = pd.read_csv(r\"X:\\Share\\OAK\\Transfer\\XChang\\MA_TAZs_with_Seaport_table.txt\")[['MA_TAZ_ID','SPRT_ID','TOWN']].merge(\n",
    "    pd.read_csv(in_dir + f)[['ID_FOR_CS','BOSTON_NB']], left_on = 'MA_TAZ_ID', right_on = 'ID_FOR_CS').groupby('SPRT_ID').first()['BOSTON_NB'].reset_index()\n",
    "\n",
    "df = town.merge(nb, how = 'left')\n",
    "df.to_csv(out_dir + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
